[38;5;4mâ„¹ Saving to output directory: training/UD_Akkadian_saa01[0m
[38;5;4mâ„¹ Using CPU[0m
[38;5;4mâ„¹ To switch to GPU 0, use the option: --gpu-id 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['tok2vec', 'trainable_lemmatizer', 'lemmatizer', 'parser',
'tagger', 'attribute_ruler', 'morphologizer'][0m
[38;5;4mâ„¹ Initial learn rate: 0.001[0m
E    #       LOSS TOK2VEC  LOSS TRAIN...  LOSS PARSER  LOSS TAGGER  LOSS MORPH...  LEMMA_ACC  DEP_UAS  DEP_LAS  SENTS_F  TAG_ACC  POS_ACC  MORPH_ACC  SCORE 
---  ------  ------------  -------------  -----------  -----------  -------------  ---------  -------  -------  -------  -------  -------  ---------  ------
  0       0          0.00         195.85        81.25       193.72         195.90      91.38    44.84     6.59    46.09    40.20    82.14      42.14    0.52
  0     200       2062.77        9345.77     12753.59      9796.36       10866.83      91.38    72.72    30.48    74.02    50.75    93.12      67.98    0.69
  0     400       4695.34        7760.84     10817.60      8644.60        9469.68      91.38    74.00    33.38    77.37    51.94    93.53      71.63    0.71
  0     600       6635.18        7364.90     12706.03      9054.85        9927.11      91.38    74.81    35.58    76.85    53.08    93.65      74.83    0.72
  1     800       7304.76        6208.34     12099.38      9632.14        9862.77      91.38    76.61    36.73    77.73    54.40    93.73      76.19    0.73
  1    1000       9707.12        6493.67     15402.72     11336.05       11652.42      91.38    77.74    37.97    79.15    54.26    93.85      76.77    0.73
  2    1200      10912.56        5425.28     16308.30     12551.05       12094.61      91.38    78.68    38.74    78.97    55.04    93.92      77.45    0.74
  3    1400      12600.41        4964.59     18127.53     14049.64       13220.26      91.38    79.76    40.54    79.15    55.31    94.00      77.83    0.75
  4    1600      14655.48        4463.09     20496.43     16083.17       13951.51      91.38    79.32    40.19    79.32    55.66    94.04      78.37    0.75
  5    1800      17229.33        4325.28     23700.18     17921.47       14918.52      91.38    79.92    40.54    79.67    55.95    94.07      79.04    0.75
  7    2000      19077.79        3872.99     27721.97     19879.16       15070.13      91.38    80.57    41.29    80.20    56.16    93.93      78.98    0.75
  9    2200      22232.04        3903.14     32858.84     22213.79       16006.20      91.38    80.18    41.27    79.91    56.30    93.95      78.91    0.75
 11    2400      25662.63        3836.34     38433.06     23920.02       16883.33      91.38    81.52    42.47    79.99    56.11    94.05      79.25    0.76
 14    2600      25099.00        3366.23     39514.27     21913.06       14952.97      91.38    81.21    41.67    80.63    55.85    93.83      79.39    0.75
 16    2800      24698.02        3024.55     41330.13     20407.06       13990.02      91.38    81.54    42.26    80.49    56.11    93.86      79.32    0.76
 19    3000      25017.71        2894.45     43112.51     18269.37       12697.02      91.38    81.50    42.09    80.81    56.16    94.00      79.27    0.76
 21    3200      24929.31        2685.80     43966.50     17133.09       11815.49      91.38    80.77    41.55    80.59    56.02    93.88      79.34    0.75
 24    3400      24768.10        2464.12     45307.59     16067.67       11143.64      91.38    81.66    42.19    80.67    56.18    93.91      79.44    0.76
 26    3600      24846.25        2288.23     46302.71     15009.73       10392.33      91.38    80.58    41.60    79.95    56.30    94.00      79.44    0.75
 29    3800      24332.40        2280.78     46827.04     14482.73       10207.82      91.38    81.18    41.83    80.38    56.30    93.91      79.37    0.75
 31    4000      23824.81        2064.88     46911.95     13382.88        9446.37      91.38    80.72    41.36    80.78    56.37    93.95      79.25    0.75
[38;5;2mâœ” Saved pipeline to output directory[0m
training/UD_Akkadian_saa01/model-last
[1m
=================================== train ===================================[0m
Running command: /Users/matthewong/miniforge3/bin/python -m spacy train configs/combined_lemmatizers_suff.cfg --output training/UD_Akkadian_saa01 --gpu-id -1 --paths.train corpus/UD_Akkadian_saa01/train --paths.dev corpus/UD_Akkadian_saa01/dev --nlp.lang=ak
