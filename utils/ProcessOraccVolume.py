import pandas as pd
import ipywidgets as widgets
import zipfile
import json
from tqdm.auto import tqdm
import os
import sys
import utils
from collections import Counter


def parsejson(text, meta_d):
    lemmas = []
    for JSONobject in text["cdl"]:
        if "cdl" in JSONobject:
            lemmas.extend(parsejson(JSONobject, meta_d))
        if "label" in JSONobject:
            meta_d["label"] = JSONobject['label']  # `label` is the line number; it stays constant until
            # the process move to a new line

        if JSONobject.get("type") == "field-start":  # this is for sign lists, identifying fields such as
            meta_d["field"] = JSONobject["subtype"]  # sign, pronunciation, translation.
        elif JSONobject.get("type") == "field-end":
            meta_d.pop("field", None)  # remove the key "field" to prevent it from being copied
            # to all subsequent lemmas (which may not have fields)
        if "f" in JSONobject:
            lemma = JSONobject["f"]
            lemma["id_word"] = JSONobject["ref"]
            lemma['label'] = meta_d["label"]
            lemma["id_text"] = meta_d["id_text"]
            if "field" in meta_d:
                lemma["field"] = meta_d["field"]
            lemmas.append(lemma)
        elif JSONobject.get("strict") == "1":  # horizontal ruling on tablet; or breakage
            lemma = {}
            lemma['extent'] = JSONobject['extent']
            lemma['scope'] = JSONobject['scope']
            lemma['state'] = JSONobject['state']
            lemma["id_word"] = JSONobject["ref"]
            lemma["id_text"] = meta_d["id_text"]
            lemmas.append(lemma)
    return lemmas


util_dir = os.path.abspath('../utils')

sys.path.append(util_dir)

os.makedirs('jsonzip', exist_ok = True)
os.makedirs('output', exist_ok = True)


#Name of volume, project to be downloaded
oraccVolume = 'saa08'
oraccProject = "saao/"+oraccVolume

#oraccProject = "saao/saa01, saao/saa02,saao/saa05,saao/saa09,saao/saa15, cams/anzu,cams/barutu,rinp/rinap4"

#Helper files generated by script, labeled according to chosen Oracc volume:

#Lemma lookup table for normalized texts
lemmaLookupFileName = 'ak_norm_lemma_lookup_' + oraccVolume + '.json' #Name of file for lemma list

#Lemma lookup table for transliterated texts
translitLookupFileName = 'akt_trans_lemma_lookup_'+oraccVolume+'.json'

#spaCy attribute ruler dictionary for normalized forms
attributeLookupFileName = 'ak_attribute_ruler_patterns_' + oraccVolume + '.json' #Name of file for attribute list


projects = widgets.Textarea(
    value=oraccProject,
    placeholder='',
    description='Projects:',
)


print(projects)

project_list = utils.format_project_list(projects.value)
project_list = utils.oracc_download(project_list)



lemm_l = [] # initiate the list that will hold all the lemmatization data of all texts in all requested projects
meta_d = {"label": None, "id_text": None}

files_lemma = [] #List that holds lemmatization data according to P-number (used for line counts below)


for project in project_list:
    file = f'jsonzip/{project.replace("/", "-")}.zip'
    try:
        zip_file = zipfile.ZipFile(file)       # create a Zipfile object
    except:
        errors = sys.exc_info() # get error information
        print(file), print(errors[0]), print(errors[1]) # and print it
        continue
    files = zip_file.namelist()     # list of all the files in the ZIP
    files = [name for name in files if "corpusjson" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.
    for filename in tqdm(files, desc = project):       #iterate over the file names
        id_text = project + filename[-13:-5] # id_text is, for instance, blms/P414332
        meta_d["id_text"] = id_text
        try:
            text_json_string = zip_file.read(filename).decode('utf-8')         #read and decode the json file of one particular text
            data_json = json.loads(text_json_string)                # make it into a json object (essentially a dictionary)
            lemmas = parsejson(data_json, meta_d)
            lemm_l.extend(lemmas)     # and send to the parsejson() function
            files_lemma.append((id_text,lemmas))
        except:
            e = sys.exc_info() # get error information
            print(filename), print(e[0]), print(e[1]) # and print it
    zip_file.close()


#Create pandas data frame
words_df = pd.DataFrame(lemm_l).fillna('')
# replace NaN (Not a Number) with empty string
#print(words_df)

findreplace = {' ' : '-', ',' : ''}
words_df = words_df.replace({'gw' : findreplace, 'sense' : findreplace}, regex = True)
#print(words_df)

#Take only the linguistic norm without word sense or POS
#words_df["lemma"] = words_df["cf"] + '[' + words_df["gw"] + ']' + words_df["pos"]
words_df["lemma"] = words_df["cf"]

words_df.loc[words_df["cf"] == "" , 'lemma'] = words_df['form'] + '[NA]NA'
words_df.loc[words_df["pos"] == "n", 'lemma'] = words_df['form'] + '[]NU'
#print(words_df[['norm', 'lemma']])

#Make dictionary out of (form, lemma) pairs from words_df
#Note here we are making the choice to have our keys be transliteration forms, not normalized forms

lemmaOutputDic = {}
for index in words_df.index:
    lemmaOutputDic.update({words_df["norm"][index]: words_df["lemma"][index]})


#print (form, lemma) pairs to JSON file
    
savefile =  open(lemmaLookupFileName,'w')
print(lemmaOutputDic, file=savefile)

#Print out dictionary of transliteration forms and their normalization

translitOutputDic = {}

for index in words_df.index:
    norm = words_df["norm"][index]
    form = words_df["form"][index]

    #If a form has no norm (e.g. 'x' or 'x+x', ums for the norm
    if norm == '':
        translitOutputDic.update({form:form})
    else:
        translitOutputDic.update({form:norm})

savefile = open(translitLookupFileName,'w')
print(translitOutputDic, file=savefile)


#Conversion dictionary between Oracc pos terms and UD.
posLabelDic = {"AN":"PROPN","CN":"PROPN","DN":"PROPN","EN":"PROPN","FN":"PROPN","GN":"PROPN","LN":"PROPN","MN":"PROPN","ON":"PROPN","PN":"PROPN","QN":"PROPN","RN":"PROPN","SN":"PROPN","TN":"PROPN","YN":"PROPN","WN":"PROPN","AJ":"ADJ","AV":"ADV","NU":"NUM","CNJ":"CONJ","DET":"DET","J":"INTJ","NP":"NOUN","N":"NOUN","PP":"PRON","V":"VERB","IP":"PRON","DP":"DET","MOD":"PART","PRP":"ADP","QP":"PRON","RP":"PRON","REL":"PRON","SBJ":"SCONJ","XP":"PRON","u":"X","n":"NUM","X":"X","NN":"NN","":""}

#List of demonstrative pronouns, needed to convert Oracc coarse pos tags for them  UD label
demPronList = ["annû","ullû","annītu","annūti","annūtu","ammūti","ammūte","annî"]

#Similar list for negation words. Oracc calls them MOD, while UD would call them NEG
negList = ["lā","ai","ul"]

#AttributeRulerList which will contain all the pattern-attribute pairs for all forms from the Oracc project
attrRulerList = []

for index in words_df.index:
    #If the token has no normalization (e.g. an x), skip
    if words_df["norm"][index] == '':
        continue

    #Otherwise, take the token and define its coarse and fine pos
    norm = words_df["norm"][index]
    lemma = words_df["lemma"][index]
    finePosType = words_df["pos"][index]
    tokenAttrDic = {}
    #print(lemma)
    #print(finePosType)
    # There are some exceptions/manual relabelings necessary between Oracc/UD

    #Reassign fine pos of ša when used in noun construct from Oracc's DET to PRP (this preserves category relation ADP > PRP)
    if lemma == "ša" and finePosType == "DET":
        tokenAttrDic.update({"POS": "ADP"})
        tokenAttrDic.update({"TAG": "PRP"})
    #Reassign coarse pos of demonstrative pronouns like annû to determiners
    elif lemma in demPronList:
        tokenAttrDic.update({"POS": "DET"})
        tokenAttrDic.update({"TAG": "DP"})
    #Reassign coarse/fine pos of šumma from Oracc's MOD/MOD (modal) to UD SCONJ/Oracc's SBJ (subordinating conjunction)
    elif lemma == "šumma":
        tokenAttrDic.update({"POS": "SCONJ"})
        tokenAttrDic.update({"TAG": "SBJ"})
    #Reassign lā from MOD/MOD to PART/NEG
    elif lemma in negList:
        tokenAttrDic.update({"POS": "PART"})
        tokenAttrDic.update({"TAG": "NEG"})
    #Otherwise, no substitution needed
    else:
        tokenAttrDic.update({"POS":posLabelDic[finePosType]})
        tokenAttrDic.update({"TAG": finePosType})
    #print(tokenAttrDic)
    #Add token's attributes to dictionary according to lemma

    patternPair = {"patterns": [[{"ORTH": norm}]]}
    attributePair = {"attrs": tokenAttrDic}
    # Add the two by adjoining second to the first
    patternPair.update(attributePair)
    # Add result to dictionary with key given by the lemma
    attrRulerList.append(patternPair)


#Get line count info for each file

log_file = open("line_count_"+oraccVolume+".csv",'w',encoding="utf-8")

for file_info in files_lemma:

    text_name = file_info[0]
    lemma_list = file_info[1]

    try:


        text_df = pd.DataFrame(lemma_list).fillna('')
        # replace NaN (Not a Number) with empty string
        # text_df.to_csv(log_file,index=True)
        #print(text_df)

        # findreplace = {' ' : '-', ',' : ''}
        # text_df = text_df.replace({'gw' : findreplace, 'sense' : findreplace}, regex = True)
        # text_df["lemma"] = text_df["cf"] + '[' + text_df["gw"] + ']' + text_df["pos"]
        # text_df.loc[text_df["cf"] == "", 'lemma'] = text_df['form'] + '[NA]NA'
        # text_df.loc[text_df["form"] == "", 'lemma'] = ""
        # text_df.loc[text_df["pos"] == "n", 'lemma'] = text_df['form'] + '[]NU'
        # text_df[['id_text', 'lemma', 'id_word', 'label']]
        # text_df["lemma"] = text_df["cf"]

        text_df.reset_index(drop=True)
        count_info = text_df[['id_word', 'label']]
        count_info.to_csv(log_file, index=True, header=False)

    except:
        print("Problem with: " + text_name)


savefile =  open(attributeLookupFileName,'w')
print(attrRulerList, file = savefile)

#Output texts in transliteration

texts_translit = words_df.groupby([words_df['id_text']]).agg({
        'form': ' '.join,
    }).reset_index()

os.makedirs('output_translit', exist_ok = True)

for idx, Q in enumerate(texts_translit["id_text"]):
    savefile =  f'{Q[-7:]}.txt'
    with open(f'output_translit/{savefile}', 'w', encoding="utf-8") as w:
        texts_translit.iloc[idx].to_csv(w, index = False, header=False)

#Now output the normalized texts

words_df["norm1"] = words_df["norm"]
words_df.loc[words_df["norm1"] == "" , 'norm1'] = words_df['form']

texts_norm = words_df.groupby([words_df['id_text']]).agg({
        'norm1': ' '.join,
    }).reset_index()

os.makedirs('output_norm', exist_ok = True)

for idx, Q in enumerate(texts_norm["id_text"]):
    savefile =  f'{Q[-7:]}.txt'
    with open(f'output_norm/{savefile}', 'w', encoding="utf-8") as w:
        texts_norm.iloc[idx].to_csv(w, index = False, header=False)


