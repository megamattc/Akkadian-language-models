import pandas as pd
import ipywidgets as widgets
import zipfile
import json
from tqdm.auto import tqdm
import os
import sys
import utils
from collections import Counter

def parse_text_json(text, id_text):
    lemmas = []
    for JSONobject in text["cdl"]:
        if "cdl" in JSONobject: 
            lemmas.extend(parse_text_json(JSONobject, id_text))
        if "f" in JSONobject:
            lemm = JSONobject["f"]
            lemm["id_text"] = id_text
            lemmas.append(lemm)
    return lemmas

util_dir = os.path.abspath('../utils')

sys.path.append(util_dir)

os.makedirs('jsonzip', exist_ok = True)
os.makedirs('output', exist_ok = True)


#Name of volume, project to be downloaded
oraccVolume = 'saa09'
oraccProject = "saao/"+oraccVolume

#Helper files generated by script, labeled according to chosen Oracc volume:

#Lemma lookup table for normalized texts
lemmaLookupFileName = 'ak_norm_lemma_lookup_' + oraccVolume + '.json' #Name of file for lemma list

#Lemma lookup table for transliterated texts
translitLookupFileName = 'akt_trans_lemma_lookup_'+oraccVolume+'.json'

#spaCy attribute ruler dictionary for normalized forms
attributeLookupFileName = 'ak_attribute_ruler_patterns_' + oraccVolume + '.json' #Name of file for attribute list


projects = widgets.Textarea(
    value=oraccProject,
    placeholder='',
    description='Projects:',
)


print(projects)

project_list = utils.format_project_list(projects.value)
project_list = utils.oracc_download(project_list)



lemm_l = [] # initiate the list that will hold all the lemmatization data of all texts in all requested projects
for project in project_list:
    file = f"jsonzip/{project.replace('/', '-')}.zip"
    try:
        zip_file = zipfile.ZipFile(file)       # create a Zipfile object
    except:
        errors = sys.exc_info() # get error information
        print(file), print(errors[0]), print(errors[1]) # and print it
        continue
        
    files = zip_file.namelist()     # list of all the files in the ZIP
    files = [name for name in files if "corpusjson" in name and name[-5:] == '.json']  

    for filename in tqdm(files, desc=project):  #iterate over the file names
        id_text = project + filename[-13:-5] # id_text is, for instance, blms/P414332
        try:
            text = zip_file.read(filename).decode('utf-8')         #read and decode the json file of one particular text
            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)
            lemm_l.extend(parse_text_json(data_json, id_text))               # and send to the parsejson() function
        except:
            errors = sys.exc_info() # get error information
            print(filename), print(errors[0]), print(errors[1]) # and print it
    zip_file.close()


#Create pandas data frame
words_df = pd.DataFrame(lemm_l).fillna('')
# replace NaN (Not a Number) with empty string
#print(words_df)

findreplace = {' ' : '-', ',' : ''}
words_df = words_df.replace({'gw' : findreplace, 'sense' : findreplace}, regex = True)
#print(words_df)

#Take only the linguistic norm without word sense or POS
#words_df["lemma"] = words_df["cf"] + '[' + words_df["gw"] + ']' + words_df["pos"]
words_df["lemma"] = words_df["cf"]

words_df.loc[words_df["cf"] == "" , 'lemma'] = words_df['form'] + '[NA]NA'
words_df.loc[words_df["pos"] == "n", 'lemma'] = words_df['form'] + '[]NU'
#print(words_df[['norm', 'lemma']])

#Make dictionary out of (form, lemma) pairs from words_df
#Note here we are making the choice to have our keys be transliteration forms, not normalized forms

lemmaOutputDic = {}
for index in words_df.index:
    lemmaOutputDic.update({words_df["norm"][index]: words_df["lemma"][index]})


#print (form, lemma) pairs to JSON file
    
savefile =  open(lemmaLookupFileName,'w')
print(lemmaOutputDic, file=savefile)

#Print out dictionary of transliteration forms and their normalization

translitOutputDic = {}

for index in words_df.index:
    norm = words_df["norm"][index]
    form = words_df["form"][index]

    #If a form has no norm (e.g. 'x' or 'x+x', ums for the norm
    if norm == '':
        translitOutputDic.update({form:form})
    else:
        translitOutputDic.update({form:norm})

savefile = open(translitLookupFileName,'w')
print(translitOutputDic, file=savefile)


#Conversion dictionary between Oracc pos terms and UD.
posLabelDic = {"AN":"PROPN","CN":"PROPN","DN":"PROPN","EN":"PROPN","FN":"PROPN","GN":"PROPN","LN":"PROPN","MN":"PROPN","ON":"PROPN","PN":"PROPN","QN":"PROPN","RN":"PROPN","SN":"PROPN","TN":"PROPN","YN":"PROPN","WN":"PROPN","AJ":"ADJ","AV":"ADV","NU":"NUM","CNJ":"CONJ","DET":"DET","J":"INTJ","N":"NOUN","PP":"PRON","V":"VERB","IP":"PRON","DP":"DET","MOD":"PART","PRP":"ADP","QP":"PRON","RP":"PRON","REL":"PRON","SBJ":"SCONJ","XP":"PRON","u":"X","n":"NUM","X":"X","NN":"NN","":""}

#List of demonstrative pronouns, needed to convert Oracc coarse pos tags for them  UD label
demPronList = ["annû","ullû","annītu","annūti","annūtu","ammūti","ammūte","annî"]

#Similar list for negation words. Oracc calls them MOD, while UD would call them NEG
negList = ["lā","ai","ul"]

#AttributeRulerList which will contain all the pattern-attribute pairs for all forms from the Oracc project
attrRulerList = []

for index in words_df.index:
    #If the token has no normalization (e.g. an x), skip
    if words_df["norm"][index] == '':
        continue

    #Otherwise, take the token and define its coarse and fine pos
    norm = words_df["norm"][index]
    lemma = words_df["lemma"][index]
    finePosType = words_df["pos"][index]
    tokenAttrDic = {}
    #print(lemma)
    #print(finePosType)
    # There are some exceptions/manual relabelings necessary between Oracc/UD

    #Reassign fine pos of ša when used in noun construct from Oracc's DET to PRP (this preserves category relation ADP > PRP)
    if lemma == "ša" and finePosType == "DET":
        tokenAttrDic.update({"POS": "ADP"})
        tokenAttrDic.update({"TAG": "PRP"})
    #Reassign coarse pos of demonstrative pronouns like annû to determiners
    elif lemma in demPronList:
        tokenAttrDic.update({"POS": "DET"})
        tokenAttrDic.update({"TAG": "DP"})
    #Reassign coarse/fine pos of šumma from Oracc's MOD/MOD (modal) to UD SCONJ/Oracc's SBJ (subordinating conjunction)
    elif lemma == "šumma":
        tokenAttrDic.update({"POS": "SCONJ"})
        tokenAttrDic.update({"TAG": "SBJ"})
    #Reassign lā from MOD/MOD to PART/NEG
    elif lemma in negList:
        tokenAttrDic.update({"POS": "PART"})
        tokenAttrDic.update({"TAG": "NEG"})
    #Otherwise, no substitution needed
    else:
        tokenAttrDic.update({"POS":posLabelDic[finePosType]})
        tokenAttrDic.update({"TAG": finePosType})
    #print(tokenAttrDic)
    #Add token's attributes to dictionary according to lemma

    patternPair = {"patterns": [[{"ORTH": norm}]]}
    attributePair = {"attrs": tokenAttrDic}
    # Add the two by adjoining second to the first
    patternPair.update(attributePair)
    # Add result to dictionary with key given by the lemma
    attrRulerList.append(patternPair)



savefile =  open(attributeLookupFileName,'w')
print(attrRulerList, file = savefile)

#Output texts in transliteration

texts_translit = words_df.groupby([words_df['id_text']]).agg({
        'form': ' '.join,
    }).reset_index()

os.makedirs('output_translit', exist_ok = True)

for idx, Q in enumerate(texts_translit["id_text"]):
    savefile =  f'{Q[-7:]}.txt'
    with open(f'output_translit/{savefile}', 'w', encoding="utf-8") as w:
        texts_translit.iloc[idx].to_csv(w, index = False, header=False)

#Now output the normalized texts

words_df["norm1"] = words_df["norm"]
words_df.loc[words_df["norm1"] == "" , 'norm1'] = words_df['form']

texts_norm = words_df.groupby([words_df['id_text']]).agg({
        'norm1': ' '.join,
    }).reset_index()

os.makedirs('output_norm', exist_ok = True)

for idx, Q in enumerate(texts_norm["id_text"]):
    savefile =  f'{Q[-7:]}.txt'
    with open(f'output_norm/{savefile}', 'w', encoding="utf-8") as w:
        texts_norm.iloc[idx].to_csv(w, index = False, header=False)
